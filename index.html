<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="GarmentPile++: Affordance-Driven Cluttered Garments Retrieval with Vision-Language Reasoning">
  <meta name="keywords" content="Cluttered Garments, VLM Reasoning, Affordance">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    GarmentPile++
  </title>

  <!-- Thumbnail for social media sharing -->
  <!-- <meta property="og:image" content="media/images/logo.png"> -->

  <!-- Favicon -->
  <!-- <link rel="icon" href="media/images/logo.png" type="image/jpeg"> -->

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">  
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
</head>
<!-- <body onload="updateInTheWild();updateBimanual();"> -->

<section class="hero">
  <div class="hero-body" style="padding-top: 1.5%;">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br>
          <img src="media/images/GarmentPile++_title.png" alt="ReKep" style="display: block; margin: 0 auto; width: 55%">
          <br>
          <img src="media/images/GarmentPile++_subtitle.png" alt="ReKep" style="display: block; margin: 0 auto; width: 70%">
          <br>
          <!-- <h1 class="title is-1 publication-title" style="color:#775295;">in Imitation Learning for Robotic Manipulation</h1> -->
          <div class="is-size-4 accepted" style="color:#000000">
            Under Review
          </div>
          <!-- <div class="is-size-4 award" style="color:#ad1010">
            Best Paper Award at Workshop on X-Embodiment Robot Learning, CoRL 2024
          </div> -->
          <!-- <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="color:#ad1010">
              <a target="_blank" href="https://github.com/wayrise">Yuran Wang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://warshallrho.github.io/">Ruihai Wu</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yuechen0614.github.io/homepage/">Yue Chen</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/Jr-kelly">Jiarui Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/SCreatorX">Jiaqi Liang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://alwaysleepy.github.io/">Ziyu Zhu</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a target="_blank" href="https://geng-haoran.github.io/">Haoran Geng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://zsdonghao.github.io/">Hao Dong</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 affiliation">
            <sup>1</sup>Peking University,
            <sup>2</sup>University of California, Berkeley
          </div>
          <br>
          <div class="affiliation-note">
            <sup>*</sup> indicates equal contributions
          </div> -->
          <!-- <div class="button-container">
            <a href="https://arxiv.org/pdf/2505.11032" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="https://wayrise.github.io/DexGarmentLab/" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="ai ai-arxiv"></i>&emsp14;Page</a>
            <a href="https://youtu.be/2S8YhBdLdww" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <a href="https://x.com/wenlong_huang/status/1829135436717142319" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="https://github.com/wayrise/DexGarmentLab" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="https://huggingface.co/datasets/wayrise/DexGarmentLab/tree/main" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="fa-light fa-folder"></i>&emsp14;Data</a>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop controls height="100%" width="100%">
            <source src="media/videos/teaser_nomask.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        We show that with proper data scaling, a single-task policy can generalize well to <strong>any new environment</strong> and 
        <strong>any new object</strong> within the same category. Remarkably, the robot can even be deployed zero-shot in a hot pot restaurant üç≤!
        </h2>
      </div>
    </div>
  </div> -->

<br>
<div class="container is-max-widescreen">
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 100%;">
    <!-- <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;
        border-bottom: 1px solid #eef;">
      Overview
    </h2> -->
    <img src="media/images/Teaser.jpg" class="method-image" style="max-width: 99%;" />
    <br>
    <p class="content has-text-justified" style="margin-bottom: 0.6em">
          <b>GarmentPile++</b> mainly includes three stages:
      <ol style="
        list-style-position: outside;
        margin-left: 1.5em;
        padding-left: 1.2em;
        text-align: left;
      ">
        <li style="margin: .6em 0;"><b>Which to Retrieve</b>: Leveraging SAM2 (with optionally triggered mask fine tuning), masks of garments are obtained to provide visual cues for aiding VLM-based reasoning on the optimal garment to retrieve under task constraints.</li>
        <li style="margin: .6em 0;"><b>Where to Retrieve</b>: The Retrieval Affordance Model infers optimal grasp points for the target garment, maximizing single-arm retrieval feasibility while ensuring the garment's safety and cleanliness.</li>
        <li style="margin: .6em 0;"><b>How to Retrieve</b>: For large/long garments, single-arm retrieval may be unfeasible. The single-arm grasping condition is therefore fed to VLM to determine whether dual-arm cooperation is required, ensuring smooth garment retrieval.</li>
      </ol>
    </p>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 75%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;
        border-bottom: 1px solid #eef;">
      Abstract
    </h2>
    <div class="content has-text-justified">
      <p>
        Garment manipulation has attracted increasing attention due to its critical role in home-assistant robotics.
        However, the majority of existing garment manipulation works assume an initial state consisting of only one garment,
        while piled garments are far more common in real-world settings.
      </p>
      <p>
        To bridge this gap, we propose a novel garment retrieval pipeline that can not only <b>follow language instruction</b> to execute <b>safe and clean retrieval</b> but also guarantee <b>exactly one garment</b> is retrieved per attempt,
        establishing a robust foundation for the execution of downstream tasks (e.g., folding, hanging, wearing).
        Our pipeline seamlessly integrates vision-language reasoning with visual affordance perception,
        fully leveraging the <b>high-level reasoning and planning capabilities of VLMs</b> alongside the generalization power of visual affordance for low-level actions.
      </p>
      <p>
        To enhance the VLM's comprehensive awareness of each garment's state within a garment pile,
        we employ visual segmentation model (SAM2) to execute <b>object segmentation</b> on the garment pile for aiding VLM-based reasoning with sufficient <b>visual cues</b>.
        A <b>mask fine-tuning mechanism</b> is further integrated to address scenarios where the initial segmentation results are suboptimal.
      </p>
      <p>
        In addition, a <b>dual-arm cooperation framework</b> is deployed to address cases involving large or long garments, as well as excessive garment sagging caused by incorrect grasping point determination, both of which are strenuous for a single arm to handle.
        The effectiveness of our pipeline are consistently demonstrated across diverse tasks and varying scenarios in both real-world and simulation environments.
      </p>
    </div>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 75%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;
        border-bottom: 1px solid #eef;">
      Video
    </h2>
    <video controls muted autoplay loop controlsList="nodownload" width="95%">
        <source src="media/videos/supp_video.mp4" type="video/mp4">
    </video>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 100%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.4em; 
        padding-bottom: 0.2em;
        border-bottom: 1px solid #eef;">
      Pipeline 
    </h2>
    <img src="media/images/Pipeline.jpg" class="method-image" />
    <p class="content has-text-justified" style="margin-bottom: 0.6em">
      Given the initial observation, <b>SAM2</b> segments the garment pile. Due to pile complexity,
      masks may be inaccurate, thus optionally triggering <b>mask fine tuning procedure</b> (Bottom-Left, red) based on <b>VLM reasoning</b>.
    </p>
    <p class="content has-text-justified" style="margin-bottom: 0.6em">
      The finalized mask and observation are then passed to the VLM to select the target garment under task constraints.
      Treating the target mask as garment pile point-cloud feature, the <b>Retrieval Affordance Model (RAM)</b> predicts single-arm grasp points,
      which is used for single-arm retrieval.
      Then the post-grasp observation is requeried by the VLM to decide on <b>dual-arm cooperation</b>;
      if required, a tracking-selection module (Bottom-Right, green) produces cooperative grasp points to ensure successful retrieval.
    </p>
    <p class="content has-text-justified" style="margin-bottom: 0.6em">
      What's more, if VLM detects the single-arm grasp of more than one garment, the current retrieval will be terminated.
    </p>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 100%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;
        border-bottom: 1px solid #eef;">
      Results
    </h2>
    <div style="text-align: left;">
      <p>
        We load 9 categories (dress, trousers, tops, skirt, socks, glove, hat, scarf and underpants) of 153 different garments from ClothesNet.
        In the Open-Boundary Scenario, 6‚Äì16 garments will be loaded to form garment piles; in the Closed-Boundary Scenario, the quantity of garments employed will be 3‚Äì8. 
        What's more, there are two types of typical tasks conducted in each scenario:
      </p>
      <ul style="list-style-type: disc; padding-left: 2em;">
        <li>
          <i>Task A: Retrieve Garments Sequentially.</i> 
          The robot is required to continue retrieving until no garments remain in the observation field.
        </li>
        <li>
          <i>Task B: Retrieve a Piece of Specific Garment.</i> 
          The robot needs to retrieve until the target garment (e.g., hat, glove, green tops, black trousers, etc.) is retrieved.
        </li>
      </ul>
    </div>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 84%;">
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Mask Fine Tuning
    </h3>
    <!-- image (default visible) -->
    <div id="staticContent_1" class="has-text-centered">
      <img src="media/images/Mask_Fine_Tuning.png" class="method-image" style="max-width: 99%;" />
    </div>
    <div style="text-align: left;">
      <p>
        Initial segmentation result (left-side images of each part) may be flawed: 
        e.g., a mask covering multiple garments (mask 1 in blue part row 1, 2; mask 1 in orange part row 1), 
        fragmented masks (mask 5 in orange part row 2; mask 4,6,7 in green part), 
        or color confusion (mask 7 in cyan part covering two white garments). 
        ‚ú© denotes the lift point for tuning. 
        After fine tuning, the adjusted masks (right-side images of each part) are clearly more reasonable, 
        with each mask generally corresponding to a single complete garment.
      </p>
    </div>
    <br>
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Affordance
    </h3>
    <!-- image (default visible) -->
    <div id="staticContent_1" class="has-text-centered">
      <img src="media/images/Affordance.png" class="method-image" style="max-width: 70%;" />
    </div>
    <div style="text-align:left;">
      <p>
        The color transition (blue ‚Üí green ‚Üí red) indicates an increasing affordance level. 
        Beyond generalization across different garment types, the affordance further captures: 
        <b>garment geometry</b> (row 1, center regions of garments are typically preferable.), 
        <b>subtle structures</b> (row 4, garment wrinkle regions easier to grasp exhibit higher affordance level.), 
        <b>spatial relations</b> (row 3 left, grasping left / right region of glove is equivalent, but the right side is preferable due to fewer garments, as well as tops side of garment in row 3 right; row 2, central regions far from closed boundaries are preferable for grasp safety.)
      </p>
    </div>
    <br>
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Reasoning Procedure
    </h3>
    <!-- image (default visible) -->
    <div id="staticContent_1" class="has-text-centered">
      <img src="media/images/Reasoning_Procedure.png" class="method-image" style="max-width: 99%;" />
    </div>
    <div style="text-align: left;">
      <p>
        For sequential garment retrieval (rows 1 and 3), the VLM infers the most suitable garment to retrieve
        (e.g., topmost or with simpler entanglement) to maximize overall success.
        For specific garment retrieval (row 2), it inters to remove obstructing garments so that the target garment is quickly exposed and ready for retrieving.
      </p>
    </div>
    <br>
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Real-World Setup
    </h3>
    <div id="staticContent_1" class="has-text-centered">
      <img src="media/images/Real_World_Setup.png" class="method-image" style="max-width: 75%;" />
    </div>
    <p>
    We use ARX x7s as real-world robot and RealSense D405 as head camera for images capture.
    </p>
  </div>
</div>
<br>
<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 100%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;
        border-bottom: 1px solid #eef;">
      Demo
    </h2>
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Simulation Videos
    </h3>
    <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; text-align: center;">
      <div style="grid-column: span 2;">
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Sequentially Retrieve (Closed)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve <span style="color: cyan;">Cyan</span> Tops (Closed)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve Glove (Closed)</h4>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/close_retrieve_sequentially_1.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/close_retrieve_sequentially_2.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/close_retrieve_cyan_tops.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/close_retrieve_glove.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br>
    <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; text-align: center;">
      <div style="grid-column: span 2;">
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Sequentially Retrieve (Open)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve <span style="color: red;">Red</span> Pants (Open)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve Striped Dress (Open)</h4>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/open_retrieve_sequentially_1.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/open_retrieve_sequentially_2.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/open_retrieve_red_pants.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/open_retrieve_striped_dress.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br>
    <h3 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 1.8em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      Real Videos
    </h3>
    <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; text-align: center;">
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Sequentially Retrieve (Closed)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve Hat (Closed)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Sequentially Retrieve (Open)</h4>
      </div>
      <div>
        <h4 style="font-weight: 700; margin-bottom: 0.5em;">Retrieve Glove (Open)</h4>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/real_close_retrieve_garments_sequentially.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/real_close_retrieve_hat.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/real_open_retrieve_garments_sequentially.mp4" type="video/mp4">
        </video>
      </div>
      <div>
        <video controls muted autoplay loop controlsList="nodownload" style="width: 100%;">
          <source src="media/videos/real_open_retrieve_glove.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</div>

<div class="columns is-centered has-text-centered">
  <div class="column" style="flex: none; width: 100%;">
    <h2 style="
        font-family: Georgia, 'Cambria', 'Times New Roman', serif;
        font-size: 2.2em; 
        font-weight: 700; 
        color: #000000; 
        margin-bottom: 0.6em;">
      BibTeX
    </h2>
    <p class="bibtex" style="text-align: left;">
      @misc{,<br>
      &nbsp;&nbsp;title={GarmentPile++: Affordance-Driven Cluttered Garments Retrieval with Vision-Language Reasoning},<br>
      &nbsp;&nbsp;author={},<br>
      &nbsp;&nbsp;year={},<br>
      &nbsp;&nbsp;eprint={},<br>
      &nbsp;&nbsp;archivePrefix={arXiv},<br>
      &nbsp;&nbsp;primaryClass={cs.RO},<br>
      &nbsp;&nbsp;url={},<br>
      }
    </p>
  </div>
</div>

</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>
            and <a href="https://rekep-robot.github.io/">ReKep</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- <script>
  document.getElementById('shuffle-video').addEventListener('click', function() {
    var envSelect = document.getElementById('env-selection');
    var taskSelect = document.getElementById('task-selection');
    var objectSelect = document.getElementById('object-selection');
    randomizeSelect(envSelect);
    randomizeSelect(taskSelect);
    randomizeSelect(objectSelect);
    console.log("Shuffle", envSelect.value, taskSelect.value, objectSelect.value)
    SelectTestVideo();
  });

  function randomizeSelect(selectElement) {
    var options = selectElement.options;
    random_move = Math.random();
    var randomIndex = Math.floor(Math.random() * options.length);
    selectElement.selectedIndex = randomIndex;
  }

  function SelectTestVideo() {
    var env_id = document.getElementById("env-selection").value;
    var task_name = document.getElementById("task-selection").value;
    var object_id = document.getElementById("object-selection").value;

    console.log("SelectTestVideo", env_id, task_name, object_id)
    var video = document.getElementById("test-video");
    video.src = "media/videos/test-wild-videos/" + env_id + "/" + 
                task_name + "/" + object_id + ".mp4";
    video.play();
  }
</script> -->

<script>
  const videoContainer = document.getElementById("video-container");
  const taskSelect = document.getElementById("task-selection");

  document.getElementById("shuffle-video").addEventListener("click", function () {
    const options = taskSelect.options;
    const randomIndex = Math.floor(Math.random() * options.length);
    taskSelect.selectedIndex = randomIndex;
    updateVideos();
  });

  function updateVideos() {
    const task = taskSelect.value;
    videoContainer.innerHTML = ""; // Ê∏ÖÁ©∫ÂΩìÂâçËßÜÈ¢ëÂå∫Âüü

    if (task === "wear_glove") {
      // Âçï‰∏™ËßÜÈ¢ëÂ±Ö‰∏≠ÊòæÁ§∫
      videoContainer.className = "columns is-centered";
      videoContainer.innerHTML = `
        <div class="column is-4 has-text-centered">
          <video controls muted autoplay loop controlsList="nodownload" width="99%">
            <source src="media/videos/data_collection/${task}/0.mp4" type="video/mp4">
          </video>
        </div>
      `;
    } else {
      // ‰∏â‰∏™ËßÜÈ¢ëÂπ∂ÊéíÊòæÁ§∫
      videoContainer.className = "columns";
      for (let i = 0; i < 3; i++) {
        const column = document.createElement("div");
        column.className = "column has-text-centered";
        column.innerHTML = `
          <video controls muted autoplay loop controlsList="nodownload" width="99%">
            <source src="media/videos/data_collection/${task}/${i}.mp4" type="video/mp4">
          </video>
        `;
        videoContainer.appendChild(column);
      }
    }
  }

  function showStatic() {
    document.getElementById('staticContent').style.display = 'block';
    document.getElementById('workflowContent').style.display = 'none';
    highlightButton('static');
  }

  function showWorkflow() {
    document.getElementById('staticContent').style.display = 'none';
    document.getElementById('workflowContent').style.display = 'block';
    highlightButton('workflow');
  }

  function highlightButton(active) {
    const btnStatic = document.getElementById('btn-static');
    const btnWorkflow = document.getElementById('btn-workflow');

    if (active === 'static') {
      btnStatic.classList.add('is-info');
      btnWorkflow.classList.remove('is-info');
    } else {
      btnWorkflow.classList.add('is-info');
      btnStatic.classList.remove('is-info');
    }
  }

  // initial Highlight Static
  highlightButton('static');
</script>

<script>
  function showStatic_1() {
    document.getElementById('staticContent_1').style.display = 'block';
    document.getElementById('workflowContent_1').style.display = 'none';
    highlightButton_1('static');
  }

  function showWorkflow_1() {
    document.getElementById('staticContent_1').style.display = 'none';
    document.getElementById('workflowContent_1').style.display = 'block';
    highlightButton_1('workflow');
  }

  function highlightButton_1(active) {
    const btnStatic_1 = document.getElementById('btn-static_1');
    const btnWorkflow_1 = document.getElementById('btn-workflow_1');

    if (active === 'static') {
      btnStatic_1.classList.add('is-info');
      btnWorkflow_1.classList.remove('is-info');
    } else {
      btnWorkflow_1.classList.add('is-info');
      btnStatic_1.classList.remove('is-info');
    }
  }

  // ÂàùÂßãÁä∂ÊÄÅÈ´ò‰∫Æ static
  highlightButton_1('static');
</script>

<script>
  function showStatic_2() {
    document.getElementById('staticContent_2').style.display = 'block';
    document.getElementById('workflowContent_2').style.display = 'none';
    highlightButton_2('static');
  }

  function showWorkflow_2() {
    document.getElementById('staticContent_2').style.display = 'none';
    document.getElementById('workflowContent_2').style.display = 'block';
    highlightButton_2('workflow');
  }

  function highlightButton_2(active) {
    const btnStatic_2 = document.getElementById('btn-static_2');
    const btnWorkflow_2 = document.getElementById('btn-workflow_2');

    if (active === 'static') {
      btnStatic_2.classList.add('is-info');
      btnWorkflow_2.classList.remove('is-info');
    } else {
      btnWorkflow_2.classList.add('is-info');
      btnStatic_2.classList.remove('is-info');
    }
  }

  // ÂàùÂßãÁä∂ÊÄÅÈ´ò‰∫Æ static
  highlightButton_2('static');
</script>

<style>
  .button.is-info.is-outlined:focus,
  .button.is-info.is-outlined:active {
    background-color: transparent;
    border-color: #ad1010;
    color: #ad1010;
    box-shadow: none;
  }

  .button.is-info.is-outlined:hover {
    background-color: #ad1010;
    color: #fff;
  }

  .button.is-info.is-outlined:hover .icon ion-icon,
  .button.is-info.is-outlined:hover span {
    color: #fff;
  }
</style>

</body>
</html>